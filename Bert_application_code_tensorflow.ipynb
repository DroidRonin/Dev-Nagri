{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn import metrics, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['root']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = !whoami\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    MAX_LEN = 140\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VALID_BATCH_SIZE=4\n",
    "    EPOCHS = 5\n",
    "    LR=0.2*3e-5\n",
    "    BERT_PATH=f\"../input/bert-uncased-multilingual-pytorch/\"\n",
    "    TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
    "    CLASS_MAPPING = {'pad':0, 'en':1, 'hi':2, 'ne':3, 'univ':4, 'acro':5}\n",
    "#     CLASS_MAPPING = {'en':0, 'hi':1}\n",
    "    MAPPING2CLASS = {i:j for j,i in CLASS_MAPPING.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./tweets_train.conll', 'r', encoding=\"utf-8\")\n",
    "    \n",
    "tokens = []\n",
    "LIDs = []\n",
    "  \n",
    "sentence_tokens = []\n",
    "sentence_lids = []\n",
    "\n",
    "for line in f:  # token level\n",
    "#        print(line)\n",
    "\n",
    "        if line != '\\n':\n",
    "            columns = line.split('\\t')  # isolate columns\n",
    "            \n",
    "        if line == '\\n':\n",
    "            # add sentences to tokens/LIDs\n",
    "            tokens.append(sentence_tokens)\n",
    "            LIDs.append(sentence_lids)\n",
    "            \n",
    "            # reset lists for next sentence\n",
    "            sentence_tokens = []\n",
    "            sentence_lids = []\n",
    "                \n",
    "#            print(columns)\n",
    "        \n",
    "        # add romanised words to sent_tokens, LIDs to sent_lids\n",
    "        else:\n",
    "            sentence_tokens.append(columns[1])\n",
    "            sentence_lids.append(columns[-2])\n",
    "\n",
    "    # create big list of [romanised words] and [LIDs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LID_mapped = []\n",
    "taggs=[]\n",
    "for i in LIDs:\n",
    "    LID_mapped.append([Config.CLASS_MAPPING.get(i, 0) for i in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, valid_tokens, train_labels, valid_labels=model_selection.train_test_split(tokens, LID_mapped, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDatasetTraining:\n",
    "    def __init__(self, query, targets, tokenizer, max_length):\n",
    "        '''\n",
    "        Dataset function used for training the notebooks. This is used for training pipelines.\n",
    "        Args:\n",
    "            query: pd.Series: The columnar series of the dataset that you want your model to train upon.\n",
    "            targets: pd.Series: The columnar series of the targets associated with your inputs.\n",
    "            tokenizer: Here we are experimenting with only two tokenizers. 1. WordSplitTokenization, 2. BytePairtokenization\n",
    "            max_length: int: What is the maximum width of sentence we want to work on. \n",
    "        '''\n",
    "        self.comment_text = query\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        This should be particularly be based on the number of datapoints in the training set.\n",
    "        '''\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        comment_text = \" \".join(self.comment_text[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        padding_length = self.max_length - len(ids)\n",
    "        \n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        targets = self.targets[item] + ([0] * (Config.MAX_LEN - len(self.targets[item])))\n",
    "#         print(ids, mask, token_type_ids, self.targets[item])\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'tokens': self.comment_text[item],\n",
    "            'targets': torch.tensor(targets, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BERTDatasetTraining(train_tokens, train_labels, Config.TOKENIZER, Config.MAX_LEN)\n",
    "train = next(iter(torch.utils.data.DataLoader(dataset, batch_size=len(train_tokens))))\n",
    "dataset = BERTDatasetTraining(valid_tokens, valid_labels, Config.TOKENIZER, Config.MAX_LEN)\n",
    "valid = next(iter(torch.utils.data.DataLoader(dataset, batch_size=len(valid_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] not gonna eat anything today cu ##z i have to wear saari ra ##at ko . [UNK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(Config.TOKENIZER.convert_ids_to_tokens(dataset[0]['ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 140, 6)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 140)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  ((None, 140, 768), (None, 167356416 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 140, 768)          0         \n",
      "_________________________________________________________________\n",
      "TimeDistLabel (TimeDistribut (None, 140, 6)            4614      \n",
      "=================================================================\n",
      "Total params: 167,361,030\n",
      "Trainable params: 167,361,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ids = tf.keras.layers.Input((Config.MAX_LEN,), dtype=tf.int32)\n",
    "att = tf.keras.layers.Input((Config.MAX_LEN,), dtype=tf.int32)\n",
    "tok = tf.keras.layers.Input((Config.MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "config = BertConfig.from_pretrained('../input/bert-multilingual-uncased-tensorflow/config.json')\n",
    "bert_model = TFBertModel.from_pretrained('../input/bert-multilingual-uncased-tensorflow/tf_model.h5',config=config)\n",
    "x = bert_model(ids)\n",
    "x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "x1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(Config.CLASS_MAPPING), activation='softmax'), name='TimeDistLabel')(x1)\n",
    "print(x1.shape)\n",
    "# x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "# x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "# x2 = tf.keras.layers.Flatten()(x2)\n",
    "# x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=ids, outputs=x1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_oh = tf.keras.utils.to_categorical(train['targets'].numpy(), len(Config.CLASS_MAPPING))\n",
    "valid_labels_oh = tf.keras.utils.to_categorical(valid['targets'].numpy(), len(Config.CLASS_MAPPING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1158 samples, validate on 290 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158/1158 [==============================] - 66s 57ms/sample - loss: 0.1448 - accuracy: 0.9468 - auc: 0.9975 - val_loss: 0.0887 - val_accuracy: 0.9694 - val_auc: 0.9988\n",
      "Epoch 2/5\n",
      "1158/1158 [==============================] - 52s 45ms/sample - loss: 0.0814 - accuracy: 0.9704 - auc: 0.9991 - val_loss: 0.0638 - val_accuracy: 0.9773 - val_auc: 0.9994\n",
      "Epoch 3/5\n",
      "1158/1158 [==============================] - 52s 45ms/sample - loss: 0.0617 - accuracy: 0.9776 - auc: 0.9994 - val_loss: 0.0551 - val_accuracy: 0.9794 - val_auc: 0.9996\n",
      "Epoch 4/5\n",
      "1158/1158 [==============================] - 52s 45ms/sample - loss: 0.0506 - accuracy: 0.9815 - auc: 0.9996 - val_loss: 0.0530 - val_accuracy: 0.9810 - val_auc: 0.9993\n",
      "Epoch 5/5\n",
      "1158/1158 [==============================] - 52s 45ms/sample - loss: 0.0400 - accuracy: 0.9855 - auc: 0.9997 - val_loss: 0.0449 - val_accuracy: 0.9844 - val_auc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7d80b5fc18>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train['ids'].numpy(), y=train_labels_oh, \n",
    "          validation_data=(valid['ids'].numpy(), valid_labels_oh),\n",
    "         batch_size=Config.TRAIN_BATCH_SIZE,\n",
    "         epochs=5,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hi', 'hi', 'hi', 'en', 'en', 'en', 'en', 'en', 'en', 'univ', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Ab na jaane kab i\\'ll be able to reach there'\n",
    "tokens = Config.TOKENIZER.encode_plus(sentence,\n",
    "                                      None,\n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=Config.MAX_LEN,\n",
    "                                    )\n",
    "padding_length = Config.MAX_LEN - len(tokens['input_ids'])\n",
    "ids = tokens['input_ids'] + ([0] * padding_length)\n",
    "inp = np.array([ids])\n",
    "res = model.predict(inp)\n",
    "print([Config.MAPPING2CLASS[i] for i in res.argmax(-1)[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
